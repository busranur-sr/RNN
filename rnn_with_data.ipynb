{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnn_with_data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOizQD4RhgFjeLLDTZ/9Gpa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/busranur-sr/RNN/blob/main/rnn_with_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import Word\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from collections import Counter\n",
        "import wordcloud\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split \n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Mw5GZVVfaSkW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the dataset\n",
        "data = pd.read_csv('Emotion_final.csv',on_bad_lines='skip')\n",
        "# Creating a new column sentiment based on overall ratings\n",
        "data.isnull().sum()\n",
        "def cleaning(df, stop_words):\n",
        "    \n",
        "    df['Text'] = df['Text'].apply(lambda x:' '.join(x.lower() for x in x.split()))\n",
        "    # Replacing the special characters\n",
        "\n",
        "    #df['verified_reviews'] = df['verified_reviews'].str.replace('[^ws]', '')\n",
        "    # Replacing the digits/numbers\n",
        "\n",
        "    df['Text'] = df['Text'].str.replace('d', '')\n",
        "\n",
        "    # Removing stop words\n",
        "\n",
        "    df['Text'] = df['Text'].apply(lambda x:' '.join(x for x in x.split() if x not in stop_words))\n",
        "\n",
        "    # Lemmatization\n",
        "    df['Text'] = df['Text'].apply(lambda x:' '.join([Word(x).lemmatize() for x in x.split()]))\n",
        "    return df\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stop_words = stopwords.words('english')\n",
        "data_v1 = cleaning(data, stop_words)\n",
        "data_v1"
      ],
      "metadata": {
        "id": "XCnNCOhHas2N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "53f67170-a2c5-4b13-fc3f-ea786ba545d9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   Text   Emotion\n",
              "0                                    int feel humiliate   sadness\n",
              "1     go feeling hopeless amne hopeful aroun someone...   sadness\n",
              "2              im grabbing minute post feel greey wrong     anger\n",
              "3     ever feeling nostalgic fireplace know still pr...      love\n",
              "4                                       feeling grouchy     anger\n",
              "...                                                 ...       ...\n",
              "9890  go church probably sit back feel awkwar talk a...   sadness\n",
              "9891  feel suspicious wrinkle prevention beauty prou...      fear\n",
              "9892                            starting feel emotional   sadness\n",
              "9893  use vegetable glycerin oil cleansing mixture n...  surprise\n",
              "9894  feel ecie friens long int come unfortunate way...       NaN\n",
              "\n",
              "[9895 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d8af1fc3-4097-4c7e-b733-855ffec48025\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Emotion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>int feel humiliate</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>go feeling hopeless amne hopeful aroun someone...</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>im grabbing minute post feel greey wrong</td>\n",
              "      <td>anger</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ever feeling nostalgic fireplace know still pr...</td>\n",
              "      <td>love</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>feeling grouchy</td>\n",
              "      <td>anger</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9890</th>\n",
              "      <td>go church probably sit back feel awkwar talk a...</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9891</th>\n",
              "      <td>feel suspicious wrinkle prevention beauty prou...</td>\n",
              "      <td>fear</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9892</th>\n",
              "      <td>starting feel emotional</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9893</th>\n",
              "      <td>use vegetable glycerin oil cleansing mixture n...</td>\n",
              "      <td>surprise</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9894</th>\n",
              "      <td>feel ecie friens long int come unfortunate way...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9895 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d8af1fc3-4097-4c7e-b733-855ffec48025')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d8af1fc3-4097-4c7e-b733-855ffec48025 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d8af1fc3-4097-4c7e-b733-855ffec48025');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoded the target column\n",
        "lb=LabelEncoder()\n",
        "data_v1['Emotion'] = lb.fit_transform(data_v1['Emotion'])\n",
        "data_v1"
      ],
      "metadata": {
        "id": "eH37JZdOa4ws",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "2b304cd8-afb1-424c-c539-1b362e854d5c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   Text  Emotion\n",
              "0                                    int feel humiliate        4\n",
              "1     go feeling hopeless amne hopeful aroun someone...        4\n",
              "2              im grabbing minute post feel greey wrong        0\n",
              "3     ever feeling nostalgic fireplace know still pr...        3\n",
              "4                                       feeling grouchy        0\n",
              "...                                                 ...      ...\n",
              "9890  go church probably sit back feel awkwar talk a...        4\n",
              "9891  feel suspicious wrinkle prevention beauty prou...        1\n",
              "9892                            starting feel emotional        4\n",
              "9893  use vegetable glycerin oil cleansing mixture n...        5\n",
              "9894  feel ecie friens long int come unfortunate way...        6\n",
              "\n",
              "[9895 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1c6a5571-3d90-4685-8116-2949d53651e6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Emotion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>int feel humiliate</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>go feeling hopeless amne hopeful aroun someone...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>im grabbing minute post feel greey wrong</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ever feeling nostalgic fireplace know still pr...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>feeling grouchy</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9890</th>\n",
              "      <td>go church probably sit back feel awkwar talk a...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9891</th>\n",
              "      <td>feel suspicious wrinkle prevention beauty prou...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9892</th>\n",
              "      <td>starting feel emotional</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9893</th>\n",
              "      <td>use vegetable glycerin oil cleansing mixture n...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9894</th>\n",
              "      <td>feel ecie friens long int come unfortunate way...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9895 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1c6a5571-3d90-4685-8116-2949d53651e6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1c6a5571-3d90-4685-8116-2949d53651e6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1c6a5571-3d90-4685-8116-2949d53651e6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=500, split=' ') \n",
        "tokenizer.fit_on_texts(data_v1['Text'].values)\n",
        "X = tokenizer.texts_to_sequences(data['Text'].values)\n",
        "X = pad_sequences(X)\n",
        "vocab_size = 500\n",
        "X[1:10]\n",
        "output_size = 5"
      ],
      "metadata": {
        "id": "Jxr4PcyPbE4O"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 100\n",
        "#seq_length = 25\n",
        "learning_rate = 1e-1\n",
        "\n",
        "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01 #input to hidden\n",
        "Whh = np.random.randn(hidden_size, hidden_size) * 0.01 #input to hidden\n",
        "Why = np.random.randn(output_size, hidden_size) * 0.01 #input to hidden\n",
        "bh = np.zeros((hidden_size, 1))\n",
        "by = np.zeros((output_size, 1))"
      ],
      "metadata": {
        "id": "04GdbZI4aGrc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def lossFun(inputs, targets, hprev):\n",
        "  \"\"\"                                                                                                                                                                                         \n",
        "  inputs,targets are both list of integers.                                                                                                                                                   \n",
        "  hprev is Hx1 array of initial hidden state                                                                                                                                                  \n",
        "  returns the loss, gradients on model parameters, and last hidden state                                                                                                                      \n",
        "  \"\"\"\n",
        "  #store our inputs, hidden states, outputs, and probability values\n",
        "  \n",
        "  for t in reversed(range(len(inputs))):\n",
        "    #output probabilities\n",
        "    dy = np.copy(ps[t])\n",
        "    #derive our first gradient\n",
        "    dy[targets[t]] -= 1 # backprop into y  \n",
        "    #compute output gradient -  output times hidden states transpose\n",
        "    #When we apply the transpose weight matrix,  \n",
        "    #we can think intuitively of this as moving the error backward\n",
        "    #through the network, giving us some sort of measure of the error \n",
        "    #at the output of the lth layer. \n",
        "    #output gradient\n",
        "    dWhy += np.dot(dy, hs[t].T)\n",
        "    #derivative of output bias\n",
        "    dby += dy\n",
        "    #backpropagate!\n",
        "    dh = np.dot(Why.T, dy) + dhnext # backprop into h                                                                                                                                         \n",
        "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity                                                                                                                     \n",
        "    dbh += dhraw #derivative of hidden bias\n",
        "    dWxh += np.dot(dhraw, xs[t].T) #derivative of input to hidden layer weight\n",
        "    dWhh += np.dot(dhraw, hs[t-1].T) #derivative of hidden layer to hidden layer weight\n",
        "    dhnext = np.dot(Whh.T, dhraw) \n",
        "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
        "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients                                                                                                                 \n",
        "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
      ],
      "metadata": {
        "id": "PkmpsPQEbjWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = open('kafka.txt', 'r').read()\n",
        "\n",
        "chars = list(set(data)) \n",
        "data_size, vocab_size = len(data), len(chars)\n",
        "print ('data has %d chars, %d unique' % (data_size, vocab_size))\n",
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsSUg0rU1IJu",
        "outputId": "81f557c3-4a39-4125-f776-3e645fba01ae"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data has 137628 chars, 80 unique\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "80"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_ix = { ch:i for i,ch in enumerate(chars)}\n",
        "ix_to_char = { i:ch for i, ch in enumerate(chars)}\n",
        "print (char_to_ix)\n",
        "print (ix_to_char)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhxgZP5R1JGr",
        "outputId": "34057231-82c2-4adc-8b3b-88d90b5aecc1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'R': 0, '9': 1, '2': 2, 'K': 3, '!': 4, '6': 5, 'A': 6, 'S': 7, 'P': 8, 'Y': 9, 'X': 10, '\\n': 11, 'q': 12, 'g': 13, 'V': 14, 'J': 15, 'ç': 16, 'o': 17, 'N': 18, 'Q': 19, 'x': 20, 'k': 21, ':': 22, 'l': 23, 'M': 24, 'T': 25, 'D': 26, '(': 27, 'h': 28, 'O': 29, 's': 30, 't': 31, '$': 32, 'W': 33, '*': 34, 'j': 35, 'u': 36, 'w': 37, 'C': 38, ';': 39, 'H': 40, '%': 41, \"'\": 42, '8': 43, '@': 44, 'i': 45, 'v': 46, '1': 47, '4': 48, 'm': 49, 'n': 50, '\"': 51, 'I': 52, '3': 53, '.': 54, 'B': 55, ')': 56, 'f': 57, '7': 58, 'd': 59, 'L': 60, 'c': 61, 'G': 62, 'E': 63, 'F': 64, ' ': 65, 'e': 66, '0': 67, '-': 68, 'p': 69, 'a': 70, 'U': 71, 'y': 72, 'z': 73, ',': 74, '5': 75, 'b': 76, '?': 77, '/': 78, 'r': 79}\n",
            "{0: 'R', 1: '9', 2: '2', 3: 'K', 4: '!', 5: '6', 6: 'A', 7: 'S', 8: 'P', 9: 'Y', 10: 'X', 11: '\\n', 12: 'q', 13: 'g', 14: 'V', 15: 'J', 16: 'ç', 17: 'o', 18: 'N', 19: 'Q', 20: 'x', 21: 'k', 22: ':', 23: 'l', 24: 'M', 25: 'T', 26: 'D', 27: '(', 28: 'h', 29: 'O', 30: 's', 31: 't', 32: '$', 33: 'W', 34: '*', 35: 'j', 36: 'u', 37: 'w', 38: 'C', 39: ';', 40: 'H', 41: '%', 42: \"'\", 43: '8', 44: '@', 45: 'i', 46: 'v', 47: '1', 48: '4', 49: 'm', 50: 'n', 51: '\"', 52: 'I', 53: '3', 54: '.', 55: 'B', 56: ')', 57: 'f', 58: '7', 59: 'd', 60: 'L', 61: 'c', 62: 'G', 63: 'E', 64: 'F', 65: ' ', 66: 'e', 67: '0', 68: '-', 69: 'p', 70: 'a', 71: 'U', 72: 'y', 73: 'z', 74: ',', 75: '5', 76: 'b', 77: '?', 78: '/', 79: 'r'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "vector_for_char_a = np.zeros((vocab_size, 1))\n",
        "vector_for_char_a[char_to_ix['a']] = 1\n",
        "print (vector_for_char_a.ravel())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsQlKTKD1MYd",
        "outputId": "570973ad-b151-47dd-83c8-616a5f08f6a4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 100\n",
        "seq_length = 25\n",
        "learning_rate = 1e-1\n",
        "\n",
        "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01 #input to hidden\n",
        "Whh = np.random.randn(hidden_size, hidden_size) * 0.01 #input to hidden\n",
        "Why = np.random.randn(vocab_size, hidden_size) * 0.01 #input to hidden\n",
        "bh = np.zeros((hidden_size, 1))\n",
        "by = np.zeros((vocab_size, 1))"
      ],
      "metadata": {
        "id": "PTzfCEEw1OOK"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p=0  \n",
        "inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
        "print (\"inputs\", inputs)\n",
        "targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
        "print (\"targets\", targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eXV3_Ls1A4A",
        "outputId": "4f1ca3e2-1239-4711-9f43-5d94bf24c6e6"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs [29, 50, 66, 65, 49, 17, 79, 50, 45, 50, 13, 74, 65, 37, 28, 66, 50, 65, 62, 79, 66, 13, 17, 79, 65]\n",
            "targets [50, 66, 65, 49, 17, 79, 50, 45, 50, 13, 74, 65, 37, 28, 66, 50, 65, 62, 79, 66, 13, 17, 79, 65, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hprev = [[0]]*100\n",
        "hprev"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRNTWG5Q1wbm",
        "outputId": "4e5d434c-e308-42a7-d13c-ba1cf93f00a9"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0],\n",
              " [0]]"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def lossFun(inputs, targets, hprev):\n",
        "  \"\"\"                                                                                                                                                                                         \n",
        "  inputs,targets are both list of integers.                                                                                                                                                   \n",
        "  hprev is Hx1 array of initial hidden state                                                                                                                                                  \n",
        "  returns the loss, gradients on model parameters, and last hidden state                                                                                                                      \n",
        "  \"\"\"\n",
        "  #store our inputs, hidden states, outputs, and probability values\n",
        "  xs, hs, ys, ps, = {}, {}, {}, {} #Empty dicts\n",
        "    # Each of these are going to be SEQ_LENGTH(Here 25) long dicts i.e. 1 vector per time(seq) step\n",
        "    # xs will store 1 hot encoded input characters for each of 25 time steps (26, 25 times)\n",
        "    # hs will store hidden state outputs for 25 time steps (100, 25 times)) plus a -1 indexed initial state\n",
        "    # to calculate the hidden state at t = 0\n",
        "    # ys will store targets i.e. expected outputs for 25 times (26, 25 times), unnormalized probabs\n",
        "    # ps will take the ys and convert them to normalized probab for chars\n",
        "    # We could have used lists BUT we need an entry with -1 to calc the 0th hidden layer\n",
        "    # -1 as  a list index would wrap around to the final element\n",
        "  xs, hs, ys, ps = {}, {}, {}, {}\n",
        "  #init with previous hidden state\n",
        "    # Using \"=\" would create a reference, this creates a whole separate copy\n",
        "    # We don't want hs[-1] to automatically change if hprev is changed\n",
        "  hs[-1] = np.copy(hprev)\n",
        "  #init loss as 0\n",
        "  loss = 0\n",
        "  # forward pass                                                                                                                                                                              \n",
        "  for t in range(len(inputs)):\n",
        "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation (we place a 0 vector as the t-th input)                                                                                                                     \n",
        "    xs[t][inputs[t]] = 1 # Inside that t-th input we use the integer in \"inputs\" list to  set the correct\n",
        "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state                                                                                                            \n",
        "  ys[1] = np.dot(Why, hs[len(inputs)-1]) + by # unnormalized log probabilities for next chars                                                                                                            \n",
        "  ps[1] = np.exp(ys[1]) / np.sum(np.exp(ys[1])) # probabilities for next chars                                                                                                              \n",
        "  loss += -np.log(ps[1][targets[len(inputs)-1],0]) # softmax (cross-entropy loss)                                                                                                                       \n",
        "    # backward pass: compute gradients going backwards    \n",
        "     #initalize vectors for gradient values for each set of weights \n",
        "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why) \n",
        "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
        "  dhnext = np.zeros_like(hs[0])\n",
        "\n",
        "  for t in reversed(range(len(inputs))):\n",
        "    t = len(inputs)-1\n",
        "    #output probabilities\n",
        "    dy = np.copy(ps[1])\n",
        "    #derive our first gradient\n",
        "    dy[targets[len(inputs)-1]] -= 1 # backprop into y  \n",
        "    #compute output gradient -  output times hidden states transpose\n",
        "    #When we apply the transpose weight matrix,  \n",
        "    #we can think intuitively of this as moving the error backward\n",
        "    #through the network, giving us some sort of measure of the error \n",
        "    #at the output of the lth layer. \n",
        "    #output gradient\n",
        "    dWhy += np.dot(dy, hs[t].T)\n",
        "    #derivative of output bias\n",
        "    dby += dy\n",
        "    #backpropagate!\n",
        "    dh = np.dot(Why.T, dy) + dhnext # backprop into h                                                                                                                                         \n",
        "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity                                                                                                                     \n",
        "    dbh += dhraw #derivative of hidden bias\n",
        "    dWxh += np.dot(dhraw, xs[t].T) #derivative of input to hidden layer weight\n",
        "    dWhh += np.dot(dhraw, hs[t-1].T) #derivative of hidden layer to hidden layer weight\n",
        "    dhnext = np.dot(Whh.T, dhraw) \n",
        "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
        "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients                                                                                                                 \n",
        "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
        "\n"
      ],
      "metadata": {
        "id": "E0ar3CyT0swW"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prediction, one full forward pass\n",
        "def sample(h, seed_ix, n):\n",
        "  \"\"\"                                                                                                                                                                                         \n",
        "  sample a sequence of integers from the model                                                                                                                                                \n",
        "  h is memory state, seed_ix is seed letter for first time step   \n",
        "  n is how many characters to predict\n",
        "  \"\"\"\n",
        "  #create vector\n",
        "  x = np.zeros((vocab_size, 1))\n",
        "  #customize it for our seed char\n",
        "  x[seed_ix] = 1\n",
        "  #list to store generated chars\n",
        "  ixes = []\n",
        "  #for as many characters as we want to generate\n",
        "  for t in range(n):\n",
        "    #a hidden state at a given time step is a function \n",
        "    #of the input at the same time step modified by a weight matrix \n",
        "    #added to the hidden state of the previous time step \n",
        "    #multiplied by its own hidden state to hidden state matrix.\n",
        "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
        "    #compute output (unnormalised)\n",
        "    y = np.dot(Why, h) + by\n",
        "    ## probabilities for next chars\n",
        "    p = np.exp(y) / np.sum(np.exp(y))\n",
        "    #pick one with the highest probability \n",
        "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
        "    #create a vector\n",
        "    x = np.zeros((vocab_size, 1))\n",
        "    #customize it for the predicted char\n",
        "    x[ix] = 1\n",
        "    #add it to the list\n",
        "    ixes.append(ix)\n",
        "\n",
        "  txt = ''.join(ix_to_char[ix] for ix in ixes)\n",
        "  print ('----\\n %s \\n----' % (txt, ))\n",
        "hprev = np.zeros((hidden_size,1)) # reset RNN memory  \n",
        "#predict the 200 next characters given 'a'\n",
        "sample(hprev,char_to_ix['a'],200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tM7YqioW1AYq",
        "outputId": "4ed7b879-eff9-422d-b246-195f588f70dc"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----\n",
            " Nb0vg,Yamçd:zGçMn)aoO$4N4;?G**E*dmmJEae@*1'.YEvE@Sefj;LL4Ve:XUFJPv?ggc\n",
            "VG2g,iyLFvW*w!.;gVjJ)4I,XL):\n",
            "BIVçTg9mUu%91xhç\"AnUh3WnvRoçAcholrTiwwC@,R,sd2n4O$JWXT\"-;uB6Wy)F@:0oaV?@jGD aTL-Kc(FQi.Q:%r0FTz4çIWl \n",
            "----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n, p = 0, 0\n",
        "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
        "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad                                                                                                                \n",
        "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0                                                                                                                        \n",
        "while n<=1000*100:\n",
        "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
        "  # check \"How to feed the loss function to see how this part works\n",
        "  if p+seq_length+1 >= len(data) or n == 0:\n",
        "    hprev = np.zeros((hidden_size,1)) # reset RNN memory                                                                                                                                      \n",
        "    p = 0 # go from start of data                                                                                                                                                             \n",
        "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
        "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
        "\n",
        "  # forward seq_length characters through the net and fetch gradient                                                                                                                          \n",
        "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
        "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "\n",
        "  # sample from the model now and then                                                                                                                                                        \n",
        "  if n % 1000 == 0:\n",
        "    print ('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
        "    sample(hprev, inputs[0], 200)\n",
        "\n",
        "  # perform parameter update with Adagrad                                                                                                                                                     \n",
        "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
        "                                [dWxh, dWhh, dWhy, dbh, dby],\n",
        "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
        "    mem += dparam * dparam\n",
        "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update                                                                                                                   \n",
        "\n",
        "  p += seq_length # move data pointer                                                                                                                                                         \n",
        "  n += 1 # iteration counter    "
      ],
      "metadata": {
        "id": "LeUFe4EbKgxJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}